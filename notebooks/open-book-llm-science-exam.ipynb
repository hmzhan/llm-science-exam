{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Large Language Model Science Exam\n\nIn this competition we are challenged to answer difficult science-based questions written by a Large Language Model. We are also told that \n\n> The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.\n\nAn idea is to make this challenge a little easier by converting it to an ***open book science exam*** using semantic search and Wikipedia.\n\n## Overview\n\n1. We obtain the plain text version of the latest dump from Wikipedia (https://www.kaggle.com/datasets/jjinho/wikipedia-20230701)\n1. We will then convert the prompts into embeddings using sentence transformers (specifically using the `all-MiniLM-L6-v2` model)\n1. We will also create embeddings of all the Wikipedia articles, and to help us, use the first sentence from each article to provide more context (again using `all-MiniLM-L6-v2`)\n1. We will then use `faiss` to perform similarity search to find the top-k articles that are most likely to have the information needed\n1. We will then get the full text of those articles and split them into sentences using the fast `blingfire` package\n1. Again, we will obtain embeddings of these sentences as well as embeddings of the prompt + answer choices and perform similarity search to get the top-k matching sentences for each question\n1. We can then combine the questions, answer choices, and context to either perform straight up question answering, or feed into a LLM","metadata":{}},{"cell_type":"markdown","source":"## TODO\n\n* ~~Enable off-line use~~\n* Improve memory efficiency\n* Make faster\n* Use context information to train a model or run inference using LLM (like https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking/notebook)","metadata":{}},{"cell_type":"markdown","source":"# Get Necessary Packages","metadata":{}},{"cell_type":"code","source":"GITHUB_TOKEN = \"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\"\nUSER = \"hmzhan\"\nCLONE_URL = f\"https://{USER}:{GITHUB_TOKEN}@github.com/{USER}/llm-science-exam.git\"\nget_ipython().system(f\"git clone {CLONE_URL}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T02:06:07.763103Z","iopub.execute_input":"2023-10-01T02:06:07.763645Z","iopub.status.idle":"2023-10-01T02:06:32.998750Z","shell.execute_reply.started":"2023-10-01T02:06:07.763613Z","shell.execute_reply":"2023-10-01T02:06:32.997591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# faiss\n!pip install -U /kaggle/input/faiss-cpu-173/faiss_cpu-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T19:04:18.788016Z","iopub.execute_input":"2023-09-30T19:04:18.788342Z","iopub.status.idle":"2023-09-30T19:04:53.040272Z","shell.execute_reply.started":"2023-09-30T19:04:18.788314Z","shell.execute_reply":"2023-09-30T19:04:53.039144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Needed otherwise encounter read-only error\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:04:53.042355Z","iopub.execute_input":"2023-09-30T19:04:53.042705Z","iopub.status.idle":"2023-09-30T19:04:55.499157Z","shell.execute_reply.started":"2023-09-30T19:04:53.042671Z","shell.execute_reply":"2023-09-30T19:04:55.497874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence transformer\n!pip install -U /kaggle/working/sentence-transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T19:04:55.500948Z","iopub.execute_input":"2023-09-30T19:04:55.501328Z","iopub.status.idle":"2023-09-30T19:05:30.534870Z","shell.execute_reply.started":"2023-09-30T19:04:55.501275Z","shell.execute_reply":"2023-09-30T19:05:30.533720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# blingfire\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-30T19:05:30.538183Z","iopub.execute_input":"2023-09-30T19:05:30.538558Z","iopub.status.idle":"2023-09-30T19:06:03.403326Z","shell.execute_reply.started":"2023-09-30T19:05:30.538521Z","shell.execute_reply":"2023-09-30T19:06:03.402226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-30T19:06:03.405260Z","iopub.execute_input":"2023-09-30T19:06:03.405860Z","iopub.status.idle":"2023-09-30T19:06:08.951224Z","shell.execute_reply.started":"2023-09-30T19:06:03.405825Z","shell.execute_reply":"2023-09-30T19:06:08.950274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code to Sentencize Text","metadata":{}},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n\n\ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-30T19:06:08.952639Z","iopub.execute_input":"2023-09-30T19:06:08.953207Z","iopub.status.idle":"2023-09-30T19:06:08.968521Z","shell.execute_reply.started":"2023-09-30T19:06:08.953174Z","shell.execute_reply":"2023-09-30T19:06:08.967559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:06:08.969956Z","iopub.execute_input":"2023-09-30T19:06:08.970272Z","iopub.status.idle":"2023-09-30T19:06:08.982228Z","shell.execute_reply.started":"2023-09-30T19:06:08.970242Z","shell.execute_reply":"2023-09-30T19:06:08.981269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:06:08.983349Z","iopub.execute_input":"2023-09-30T19:06:08.984042Z","iopub.status.idle":"2023-09-30T19:06:09.001868Z","shell.execute_reply.started":"2023-09-30T19:06:08.984003Z","shell.execute_reply":"2023-09-30T19:06:09.001082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:06:09.002966Z","iopub.execute_input":"2023-09-30T19:06:09.003695Z","iopub.status.idle":"2023-09-30T19:06:09.022893Z","shell.execute_reply.started":"2023-09-30T19:06:09.003663Z","shell.execute_reply":"2023-09-30T19:06:09.022100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:06:09.026290Z","iopub.execute_input":"2023-09-30T19:06:09.026681Z","iopub.status.idle":"2023-09-30T19:06:17.612467Z","shell.execute_reply.started":"2023-09-30T19:06:09.026658Z","shell.execute_reply":"2023-09-30T19:06:17.611521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using precomputed index of the Wikipedia 2023-07 dump\n\nDataset can be found: https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index","metadata":{}},{"cell_type":"code","source":"sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:06:17.613748Z","iopub.execute_input":"2023-09-30T19:06:17.614119Z","iopub.status.idle":"2023-09-30T19:07:30.332638Z","shell.execute_reply.started":"2023-09-30T19:06:17.614084Z","shell.execute_reply":"2023-09-30T19:07:30.330795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode the prompts\n\nWe observe that the prompts contain the subject matter almost always at the end. Here we just encode the entire prompt using `sentence_transformers` so that we can use semantic search to find appropriate articles that may have information relating to the questions.","metadata":{}},{"cell_type":"code","source":"prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True).half()\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:07:30.335198Z","iopub.execute_input":"2023-09-30T19:07:30.336157Z","iopub.status.idle":"2023-09-30T19:07:38.070154Z","shell.execute_reply.started":"2023-09-30T19:07:30.336119Z","shell.execute_reply":"2023-09-30T19:07:38.069215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:07:38.071332Z","iopub.execute_input":"2023-09-30T19:07:38.072244Z","iopub.status.idle":"2023-09-30T19:07:38.371795Z","shell.execute_reply.started":"2023-09-30T19:07:38.072192Z","shell.execute_reply":"2023-09-30T19:07:38.370645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the top 3 pages that are likely to contain the topic of interest\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:07:38.373630Z","iopub.execute_input":"2023-09-30T19:07:38.374501Z","iopub.status.idle":"2023-09-30T19:07:57.664736Z","shell.execute_reply.started":"2023-09-30T19:07:38.374466Z","shell.execute_reply":"2023-09-30T19:07:57.663981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save memory - delete sentence_index since it is no longer necessary\ndel sentence_index\ndel prompt_embeddings\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:07:57.666232Z","iopub.execute_input":"2023-09-30T19:07:57.666893Z","iopub.status.idle":"2023-09-30T19:07:58.825618Z","shell.execute_reply.started":"2023-09-30T19:07:57.666860Z","shell.execute_reply":"2023-09-30T19:07:58.824624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load the Wikipedia Index File","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:07:58.826935Z","iopub.execute_input":"2023-09-30T19:07:58.828120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the article and associated file location using the index\nwikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    ## Get indices by score threshold\n    #scr_idx = idx[np.where(scr <= 0.85)]\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.sort_values(by=\"prompt_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikipedia_file_data.file.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the full text data\nwiki_text_data = []\n\nfor file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n    _df = _df[_df['id'].isin(_id)]\n    wiki_text_data.append(_df)\n    _ = gc.collect()\nwiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_text_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split full-text Wikipedia Documents into Sentences\n\nWe split the Wikipedia documents into sentences because we can observe that in many cases it seems that GPT3.5 directly took the answers from the text. We therefore want to retrieve the most similar sentences to provide context.","metadata":{}},{"cell_type":"code","source":"## Parse documents into sentences\nprocessed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:21.435514Z","iopub.execute_input":"2023-09-30T01:57:21.436409Z","iopub.status.idle":"2023-09-30T01:57:23.927116Z","shell.execute_reply.started":"2023-09-30T01:57:21.436379Z","shell.execute_reply":"2023-09-30T01:57:23.926212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_wiki_text_data","metadata":{"execution":{"iopub.status.busy":"2023-09-30T02:04:39.796638Z","iopub.execute_input":"2023-09-30T02:04:39.797031Z","iopub.status.idle":"2023-09-30T02:04:39.810144Z","shell.execute_reply.started":"2023-09-30T02:04:39.797001Z","shell.execute_reply":"2023-09-30T02:04:39.808968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get embeddings of the wiki text data\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True).half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:23.928391Z","iopub.execute_input":"2023-09-30T01:57:23.929230Z","iopub.status.idle":"2023-09-30T01:57:44.735234Z","shell.execute_reply.started":"2023-09-30T01:57:23.929195Z","shell.execute_reply":"2023-09-30T01:57:44.734237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:44.736583Z","iopub.execute_input":"2023-09-30T01:57:44.737562Z","iopub.status.idle":"2023-09-30T01:57:45.033563Z","shell.execute_reply.started":"2023-09-30T01:57:44.737527Z","shell.execute_reply":"2023-09-30T01:57:45.032520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_data_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:45.034769Z","iopub.execute_input":"2023-09-30T01:57:45.035665Z","iopub.status.idle":"2023-09-30T01:57:45.047636Z","shell.execute_reply.started":"2023-09-30T01:57:45.035627Z","shell.execute_reply":"2023-09-30T01:57:45.046527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Found that encoding the prompt and the answers gave the best quality results for retrieval.","metadata":{}},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:45.049126Z","iopub.execute_input":"2023-09-30T01:57:45.049486Z","iopub.status.idle":"2023-09-30T01:57:45.067950Z","shell.execute_reply.started":"2023-09-30T01:57:45.049434Z","shell.execute_reply":"2023-09-30T01:57:45.066931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True).half()\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:45.073754Z","iopub.execute_input":"2023-09-30T01:57:45.074422Z","iopub.status.idle":"2023-09-30T01:57:45.396379Z","shell.execute_reply.started":"2023-09-30T01:57:45.074391Z","shell.execute_reply":"2023-09-30T01:57:45.395469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameter to determine how many relevant sentences to include\nNUM_SENTENCES_INCLUDE = 3\n\n## List containing Question, Choices, Context\nprompt_contexts = []\n\n## List containing just Context\ncontexts = []\n\nfor r in trn.itertuples():\n    prompt_context = \"\"\n\n    prompt_id = r.id\n\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n    prompt_context += \"Question: \" + trn.prompt.iloc[prompt_id] + \"\\n\"\n\n    prompt_context += \"Choices:\\n\"\n    prompt_context += \"(A) \" + trn.A.iloc[prompt_id] + \"\\n\"\n    prompt_context += \"(B) \" + trn.B.iloc[prompt_id] + \"\\n\"\n    prompt_context += \"(C) \" + trn.C.iloc[prompt_id] + \"\\n\"\n    prompt_context += \"(D) \" + trn.D.iloc[prompt_id] + \"\\n\"\n    prompt_context += \"(E) \" + trn.E.iloc[prompt_id] + \"\\n\"\n\n    if prompt_indices.shape[0] > 0:\n        prompt_context += \"Context:\\n\"\n        ## Per Prompt Index\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"\n        \n        ## Get the top matches\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            ## Threshold on the score\n            if _s < 2:\n                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \"\\n\"\n        prompt_context += context\n        \n    contexts.append(context)\n    prompt_contexts.append(prompt_context)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:45.397622Z","iopub.execute_input":"2023-09-30T01:57:45.398433Z","iopub.status.idle":"2023-09-30T01:57:46.421469Z","shell.execute_reply.started":"2023-09-30T01:57:45.398398Z","shell.execute_reply":"2023-09-30T01:57:46.420720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contexts[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T02:05:41.047583Z","iopub.execute_input":"2023-09-30T02:05:41.047921Z","iopub.status.idle":"2023-09-30T02:05:41.054479Z","shell.execute_reply.started":"2023-09-30T02:05:41.047895Z","shell.execute_reply":"2023-09-30T02:05:41.053374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_contexts[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T02:05:57.768293Z","iopub.execute_input":"2023-09-30T02:05:57.768660Z","iopub.status.idle":"2023-09-30T02:05:57.775901Z","shell.execute_reply.started":"2023-09-30T02:05:57.768632Z","shell.execute_reply":"2023-09-30T02:05:57.774840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts","metadata":{"execution":{"iopub.status.busy":"2023-09-30T01:57:46.424237Z","iopub.execute_input":"2023-09-30T01:57:46.425003Z","iopub.status.idle":"2023-09-30T01:57:46.431390Z","shell.execute_reply.started":"2023-09-30T01:57:46.424968Z","shell.execute_reply":"2023-09-30T01:57:46.430725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn.to_csv(\"./train_context.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:31:13.823929Z","iopub.execute_input":"2023-09-24T16:31:13.825142Z","iopub.status.idle":"2023-09-24T16:31:13.868422Z","shell.execute_reply.started":"2023-09-24T16:31:13.825106Z","shell.execute_reply":"2023-09-24T16:31:13.867729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Open Book Test Taking!\n\nBelow we can see some results which provides not only the question and choices, but also context from Wikipedia which may provide crucial hints or even the answers themselves!","metadata":{}},{"cell_type":"code","source":"for i, p in enumerate(prompt_contexts[:10]):\n    print(f\"Question {i}\")\n    print(p)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T18:54:23.266653Z","iopub.execute_input":"2023-07-17T18:54:23.267268Z","iopub.status.idle":"2023-07-17T18:54:23.273947Z","shell.execute_reply.started":"2023-07-17T18:54:23.267232Z","shell.execute_reply":"2023-07-17T18:54:23.272829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}